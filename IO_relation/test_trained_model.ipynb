{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11427906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, AutoTokenizer\n",
    "from model import RelModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e87fb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "913f9893",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b8a7151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "encoder = RobertaModel.from_pretrained(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f4fe9aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RelModel(encoder).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c902bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"/home/ubuntu/nlm/nima/Data/Models/boolq_for_rel/BioRel-short/exp2/epoch_17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b7302b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "88978c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b6b9b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/ubuntu/nlm/nima/Data/BioRel/dev.json\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "52bf285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 52321\n",
    "input_ = tokenizer(\" \".join(data[i]['relation'].split(\"_\")), data[i]['sentence'], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c74630ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = torch.load('/home/ubuntu/nlm/nima/Data/BioRel/IO-pre-processed-huggingface/type3/input_ids_dev.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0f20ba78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(43)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(dev_data[0] == 1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fe3dc0fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-e93fdae303e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(dev_data):\n",
    "    m = torch.where(d == 1)[0][0]\n",
    "    input_ = {\n",
    "        \"input_ids\": d[:m].unsqueeze(0).to('cuda'), \n",
    "        \"attention_mask\": torch.ones([1,m]).to('cuda')\n",
    "    }\n",
    "        \n",
    "    head_logit, tail_logit = model(input_)\n",
    "    head_mask = head_logit.argmax(axis = -1).squeeze()\n",
    "    tail_mask = tail_logit.argmax(axis = -1).squeeze()\n",
    "    print(head_mask.sum(), tail_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a3ca334e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   417,  1496,  3175,    34,  3059, 41115, 30344,  1082,     2,\n",
       "             2,   627,  2225,  2656,    19,     5,  3169,     8,   251,    12,\n",
       "          1279,   775,     9,  1416,     9, 30011,  2182,     8,   112,     6,\n",
       "         21190,  2943,  1484,    19, 10665,  1668,  7187,    15,    23,     5,\n",
       "          1494,     9, 10665,     8, 18422,  1988,  6204, 23991,     9,     5,\n",
       "         14619,    11,  7571,    12, 43768,   479,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a57a6c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  6403, 38315, 26387,   222,    45,  3625,  3327,  2474,    12,\n",
       "          9981,   405,  2408,  1022,  2156,    53,  1130,    36,   181,   540,\n",
       "            87,   479,  2546,  4839,  1230,  2408,  3077,    23,    70,    86,\n",
       "          5788,    11,   258,  7341,   479,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(data[0]['sentence'], return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a61767a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 64112\n",
    "input_ = tokenizer(data[i]['relation'], data[i]['sentence'], return_tensors='pt').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "261b2a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_logit, tail_logit = model(input_)\n",
    "head_mask = head_logit.argmax(axis = -1).squeeze()\n",
    "tail_mask = tail_logit.argmax(axis = -1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2b456c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' thrombocytopenia'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_.input_ids[0][head_mask.bool()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "689715a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' heparin'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_.input_ids[0][tail_mask.bool()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4778d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "26b776cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'head': {'CUI': 'C0040034',\n",
       "  'word': 'thrombocytopenia',\n",
       "  'start': 107,\n",
       "  'length': 16,\n",
       "  'split_start': 17},\n",
       " 'tail': {'CUI': 'C0019134',\n",
       "  'word': 'heparin',\n",
       "  'start': 233,\n",
       "  'length': 7,\n",
       "  'split_start': 36},\n",
       " 'sentence': 'careful clinical review suggested that heparin was either the most likely cause of a contributing cause of thrombocytopenia in 5/166 patients ( 3.0 % ) receiving therapeutic heparin and none of the patients who received prophylactic heparin .',\n",
       " 'relation': 'has_contraindicated_drug',\n",
       " 'lexical_feature0': 'of|thrombocytopenia|in|5/166|patients|(|3.0|%|)|receiving|therapeutic|heparin|and|none|of|the|patients|who|received|prophylactic|heparin|.',\n",
       " 'lexical_feature1': 'cause|of|thrombocytopenia|in|5/166|patients|(|3.0|%|)|receiving|therapeutic|heparin|and|none|of|the|patients|who|received|prophylactic|heparin|.|PAD',\n",
       " 'lexical_feature2': 'contributing|cause|of|thrombocytopenia|in|5/166|patients|(|3.0|%|)|receiving|therapeutic|heparin|and|none|of|the|patients|who|received|prophylactic|heparin|.|PAD|PAD',\n",
       " 'syntactic_feature0': 'thrombocytopenia|9|nsubj',\n",
       " 'syntactic_feature1': 'in|4|case',\n",
       " 'syntactic_feature2': '5/166|4|nummod',\n",
       " 'syntactic_feature3': 'patients|1|nmod',\n",
       " 'syntactic_feature4': '(|7|punct',\n",
       " 'syntactic_feature5': '3.0|7|nummod',\n",
       " 'syntactic_feature6': '%|4|appos',\n",
       " 'syntactic_feature7': ')|7|punct',\n",
       " 'syntactic_feature8': 'receiving|0|root',\n",
       " 'syntactic_feature9': 'therapeutic|11|amod',\n",
       " 'syntactic_feature10': 'heparin|9|obj',\n",
       " 'syntactic_feature11': 'and|13|cc',\n",
       " 'syntactic_feature12': 'none|11|conj',\n",
       " 'syntactic_feature13': 'of|16|case',\n",
       " 'syntactic_feature14': 'the|16|det',\n",
       " 'syntactic_feature15': 'patients|13|nmod',\n",
       " 'syntactic_feature16': 'who|18|nsubj',\n",
       " 'syntactic_feature17': 'received|16|acl:relcl',\n",
       " 'syntactic_feature18': 'prophylactic|20|amod',\n",
       " 'syntactic_feature19': 'heparin|18|obj'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0fec1047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>disease has associated anatomic site</s></s>the paper deals with the immediate and long-term results of treatment of 173 female and 1,330 male patients with lung cancer operated on at the department of lung and mediastinal tumors of the institute in 1960-1979.</s>'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba7e3db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,   354,  4760,    30, 10596,  1152,     2,     2,  6403, 38315,\n",
       "         26387,   222,    45,  3625,  3327,  2474,    12,  9981,   405,  2408,\n",
       "          1022,  2156,    53,  1130,    36,   181,   540,    87,   479,  2546,\n",
       "          4839,  1230,  2408,  3077,    23,    70,    86,  5788,    11,   258,\n",
       "          7341,   479,     2]),\n",
       " 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4907e73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_anaconda3)",
   "language": "python",
   "name": "conda_anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
